```{r setup_city_catboost_m2_SHAP, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
# library(xgboost)
library(catboost)
library(caret)
library(dplyr)
library(pROC)
library(ROCR)
library(themis)
library(recipes)
library(MLmetrics)
library(knitr)
library(lubridate)
library(tidyr) # Added tidyr for pivot_wider

# df_resort <- read.csv("resort_hotel.csv")
df_city <- read.csv("city_hotel.csv")

set.seed(42)
```

### City Hotel

```{r echo=FALSE, message=FALSE, warning=FALSE}
df_hotel <- df_city
df_hotel$arrival_date <- as.Date(df_hotel$arrival_date)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(rsample)

# ==== Step 1: Random Stratified Split for Method II ====
set.seed(42)

split <- initial_split(df_hotel, prop = 0.75, strata = "is_canceled")
train_data <- training(split)
test_data <- testing(split)

```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# ==== Step 2: Remove Unused Columns ====
drop_columns <- c("hotel", "arrival_date", "year_month", "days_in_waiting_list", "arrival_date_year", "assigned_room_type", "booking_changes", "reservation_status", "country", "days_in_waiting_list")
train_data <- train_data %>% select(-any_of(drop_columns))
test_data_for_model <- test_data %>% select(-arrival_date)  # test_data 沒 year_month
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# 中位數填補數值型 NA
num_cols <- sapply(train_data, is.numeric)
for (col in names(train_data)[num_cols]) {
  med <- median(train_data[[col]], na.rm = TRUE)
  train_data[[col]][is.na(train_data[[col]])] <- med
  test_data_for_model[[col]][is.na(test_data_for_model[[col]])] <- med
}

# 眾數填補類別型 NA
cat_cols <- sapply(train_data, is.factor)
for (col in names(train_data)[cat_cols]) {
  mode_val <- names(sort(table(train_data[[col]]), decreasing = TRUE))[1]
  train_data[[col]][is.na(train_data[[col]])] <- mode_val
  test_data_for_model[[col]][is.na(test_data_for_model[[col]])] <- mode_val
}

```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# ==== Step 3: Target Label Formatting ====
train_data$is_canceled <- factor(train_data$is_canceled, levels = c(0, 1), labels = c("not_canceled", "canceled"))
test_data_for_model$is_canceled <- factor(test_data_for_model$is_canceled, levels = c(0, 1), labels = c("not_canceled", "canceled"))
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# ==== Step 4: Select Features Using SHAP (CatBoost) ====

# 清除 NA
train_complete <- train_data[complete.cases(train_data), ]

# 處理類別欄位
categorical_cols <- names(train_complete)[sapply(train_complete, function(x) is.character(x) || is.factor(x))]
categorical_cols <- setdiff(categorical_cols, "is_canceled")
train_complete[categorical_cols] <- lapply(train_complete[categorical_cols], as.factor)

# 建立 Pool
train_pool <- catboost.load_pool(
  data = subset(train_complete, select = -is_canceled),
  label = train_complete$is_canceled
)

# 訓練模型
model_cat <- catboost.train(train_pool, params = list(
  loss_function = 'Logloss',
  iterations = 50,
  depth = 3,
  learning_rate = 0.1,
  random_seed = 42,
  verbose = 0
))

# 計算 SHAP 值（每個 row * 每個 feature）
shap_matrix <- catboost.get_feature_importance(model_cat, pool = train_pool, type = "ShapValues")

# 最後一欄是 bias term，要去掉
shap_matrix <- shap_matrix[, -ncol(shap_matrix)]

# 計算每個特徵的平均 SHAP 值（全體觀測值平均絕對值）
mean_shap <- apply(abs(shap_matrix), 2, mean)
feature_names <- colnames(subset(train_complete, select = -is_canceled))

importance_df <- data.frame(Feature = feature_names, Importance = mean_shap)

# 選擇前 10 名重要特徵
top_features <- importance_df %>%
  arrange(desc(Importance)) %>%
  slice(1:min(10, n())) %>%
  pull(Feature)

top_features <- intersect(top_features, colnames(train_data))
print(top_features)

```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# ==== Step 5: Select Features ====
select_features <- function(df) df[, c(top_features, "is_canceled")]
train_selected <- select_features(train_data)
test_selected <- select_features(test_data_for_model)
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
train_model_with_catboost <- function(train_df, label_name, top_features, sampling_method = c("none", "smote", "down"), seed = 42) {
  library(catboost)
  library(dplyr)
  library(smotefamily)
  
  # 設定 CatBoost 避免 logger 警告
  options(catboost.verbose = FALSE)
  
  set.seed(seed)
  sampling_method <- match.arg(sampling_method)
  
  # ========== 防呆檢查 ==========
  if (!(label_name %in% colnames(train_df))) stop("Label column not found.")
  if (nrow(train_df) == 0) stop("Input data has no rows.")
  if (length(top_features) == 0) stop("top_features is empty.")
  
  # ========== Label 處理 ==========
  label_raw <- train_df[[label_name]]
  
  # 統一轉換為 0/1 binary（與你的 XGBoost 邏輯一致）
  if (is.factor(label_raw)) {
    # 如果已經是 factor，轉為 0/1
    label <- ifelse(label_raw == "canceled", 1, 0)
  } else if (is.character(label_raw)) {
    label <- ifelse(label_raw == "canceled", 1, 0)
  } else if (is.numeric(label_raw)) {
    label <- ifelse(label_raw == 1, 1, 0)
  } else {
    stop("Unsupported label type")
  }
  
  if (any(is.na(label))) stop("Label column contains NA.")
  
  # ========== 特徵處理 ==========
  model_data <- train_df[, c(top_features, label_name)]
  model_data[[label_name]] <- label
  
  # 處理特徵類型
  for (feature in top_features) {
    if (is.character(model_data[[feature]])) {
      model_data[[feature]] <- as.factor(model_data[[feature]])
    } else if (is.integer(model_data[[feature]])) {
      model_data[[feature]] <- as.numeric(model_data[[feature]])
    }
  }
  
  # 移除含 NA 的行
  model_data <- model_data[complete.cases(model_data), ]
  if (nrow(model_data) == 0) stop("No complete cases after removing NA.")
  
  # ========== 應用採樣方法 ==========
  if (sampling_method == "smote") {
    label_counts <- table(model_data[[label_name]])
    minority_count <- min(label_counts)
    majority_count <- max(label_counts)
    
    if (minority_count <= 5) {
      warning("Too few samples in minority class for SMOTE. Using original data.")
    } else {
      X_data <- model_data[, top_features, drop = FALSE]
      y_data <- as.factor(model_data[[label_name]])
      
      K_value <- min(5, minority_count - 1)
      
      tryCatch({
        smote_result <- SMOTE(
          X = X_data,
          target = y_data,
          K = K_value,
          dup_size = ceiling((majority_count - minority_count) / minority_count)
        )
        
        model_data <- smote_result$data
        model_data[[label_name]] <- as.numeric(as.character(model_data$class))
        model_data$class <- NULL
        
      }, error = function(e) {
        warning(paste("SMOTE failed:", e$message, ". Using original data."))
      })
    }
    
  } else if (sampling_method == "down") {
    n_min <- min(table(model_data[[label_name]]))
    
    model_data <- model_data %>%
      group_by(!!sym(label_name)) %>%
      sample_n(n_min, replace = FALSE) %>%
      ungroup() %>%
      as.data.frame()
  }
  
  # ========== 3-Fold Cross Validation ==========
  set.seed(seed)
  n_folds <- 3
  n_obs <- nrow(model_data)
  fold_indices <- sample(rep(1:n_folds, length.out = n_obs))
  
  cv_results <- data.frame(
    Fold = integer(),
    Accuracy = numeric(),
    AUC = numeric(),
    stringsAsFactors = FALSE
  )
  
  # 儲存每個 fold 的模型（可選）
  fold_models <- list()
  metrics_list <- list() # conf
  
  for (fold in 1:n_folds) {
    # 分割訓練和驗證集
    train_idx <- which(fold_indices != fold)
    val_idx <- which(fold_indices == fold)
    
    train_fold <- model_data[train_idx, ]
    val_fold <- model_data[val_idx, ]
    
    # 建立 CatBoost Pool
    cat_features <- which(sapply(train_fold[, top_features, drop = FALSE], is.factor)) - 1
    
    train_pool <- catboost.load_pool(
      data = train_fold[, top_features, drop = FALSE],
      label = train_fold[[label_name]],
      cat_features = if(length(cat_features) > 0) cat_features else NULL
    )
    
    val_pool <- catboost.load_pool(
      data = val_fold[, top_features, drop = FALSE],
      label = val_fold[[label_name]],
      cat_features = if(length(cat_features) > 0) cat_features else NULL
    )
    
    # CatBoost 參數
    params <- list(
      loss_function = 'Logloss',
      eval_metric = 'AUC',
      iterations = 100,
      depth = 6,
      learning_rate = 0.1,
      random_seed = seed,
      verbose = 0,
      logging_level = 'Silent'
    )
    
    # 訓練模型
    fold_model <- catboost.train(train_pool, params = params)
    fold_models[[fold]] <- fold_model
    
    # 在驗證集上預測
    val_pred_prob <- catboost.predict(fold_model, val_pool, prediction_type = 'Probability')
    val_pred_class <- ifelse(val_pred_prob > 0.5, 1, 0)
    
    # 計算性能指標
    val_actual <- val_fold[[label_name]]
    fold_accuracy <- mean(val_pred_class == val_actual)
    
    # 計算 AUC
    if (requireNamespace("pROC", quietly = TRUE)) {
      fold_auc <- pROC::auc(val_actual, val_pred_prob)
    } else {
      fold_auc <- NA
    }
    
    # 混淆矩陣指標
    conf <- confusionMatrix(
      factor(val_pred_class, levels = c(0, 1)),
      factor(val_actual, levels = c(0, 1)),
      positive = "1"
    )
    
    fold_precision <- conf$byClass["Precision"]
    fold_recall <- conf$byClass["Recall"]
    fold_f1 <- conf$byClass["F1"]

    
    # 儲存結果
    # cv_results <- rbind(cv_results, data.frame(
    #   Fold = fold,
    #   Accuracy = fold_accuracy,
    #   AUC = as.numeric(fold_auc),
    #   stringsAsFactors = FALSE
    # ))
    cv_results <- rbind(cv_results, data.frame(
      Fold = fold,
      Accuracy = fold_accuracy,
      AUC = as.numeric(fold_auc),
      Precision = fold_precision,
      Recall = fold_recall,
      F1_Score = fold_f1,
      stringsAsFactors = FALSE
    ))
  }
  
  # ========== 計算平均 CV 性能 ==========
  mean_accuracy <- mean(cv_results$Accuracy, na.rm = TRUE)
  mean_auc <- mean(cv_results$AUC, na.rm = TRUE)
  mean_precision <- mean(cv_results$Precision, na.rm = TRUE) # conf
  mean_recall    <- mean(cv_results$Recall, na.rm = TRUE) # conf
  mean_f1        <- mean(cv_results$F1_Score, na.rm = TRUE) # conf

  
  # ========== 用全部資料訓練最終模型 ==========
  cat_features <- which(sapply(model_data[, top_features, drop = FALSE], is.factor)) - 1
  
  final_pool <- catboost.load_pool(
    data = model_data[, top_features, drop = FALSE],
    label = model_data[[label_name]],
    cat_features = if(length(cat_features) > 0) cat_features else NULL
  )
  
  params <- list(
    loss_function = 'Logloss',
    eval_metric = 'AUC',
    iterations = 100,
    depth = 6,
    learning_rate = 0.1,
    random_seed = seed,
    verbose = 0,
    logging_level = 'Silent'
  )
  
  final_model <- catboost.train(final_pool, params = params)
  
  # ========== 返回結果 ==========
  result <- list(
    # 最終模型
    finalModel = final_model,
    
    # CV 結果（替代 getTrainPerf）
    cv_results = cv_results,
    # cv_performance = data.frame(
    #   Accuracy = mean_accuracy,
    #   AUC = mean_auc,
    #   stringsAsFactors = FALSE
    # ),
    cv_performance <- data.frame(
      Accuracy = mean_accuracy,
      AUC = mean_auc,
      Precision = mean_precision,
      Recall = mean_recall,
      F1_Score = mean_f1,
      stringsAsFactors = FALSE
    ),

    
    # 其他資訊
    method = "catboost",
    sampling_method = sampling_method,
    features = top_features,
    label_name = label_name,
    training_data = model_data,
    fold_models = fold_models,
    
    # 模擬 caret 結構（如果需要）
    bestTune = data.frame(
      iterations = params$iterations,
      depth = params$depth,
      learning_rate = params$learning_rate,
      stringsAsFactors = FALSE
    )
  )
  
  class(result) <- c("catboost_cv", "list")
  return(result)
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# model_smote <- train_model_with_catboost(train_data, "is_canceled", top_features, sampling_method = "smote")
# model_downsample  <- train_model_with_catboost(train_data, "is_canceled", top_features, sampling_method = "down")
# 
# perf_smote <- getTrainPerf(model_smote)
# perf_down <- getTrainPerf(model_downsample)

result_model_smote <- train_model_with_catboost(train_data, "is_canceled", top_features, sampling_method = "smote")
model_smote <- result_model_smote$finalModel

result_model_downsample <- train_model_with_catboost(train_data, "is_canceled", top_features, sampling_method = "down")
model_downsample <- result_model_downsample$finalModel

perf_smote <- result_model_smote$cv_performance
perf_down <- result_model_smote$cv_performance
```

---

```{r echo=FALSE, message=FALSE, warning=FALSE}
evaluate_catboost_model <- function(model, data_df, top_features, label_col = "is_canceled", threshold = 0.65) {
  # 選出特徵欄位並處理類別轉型
  predict_data <- data_df[, top_features, drop = FALSE]

  # 確保所有類別欄轉為 factor（CatBoost 需要）
  predict_data[] <- lapply(predict_data, function(x) {
    if (is.character(x)) return(as.factor(x))
    else return(x)
  })

  # 建立 Pool，這邊不需要指定 cat_features，CatBoost 會自動辨識 factor
  pool <- catboost.load_pool(data = predict_data)

  # 機率預測 + 二元分類
  probs <- catboost.predict(model, pool, prediction_type = "Probability")
  preds <- ifelse(probs > threshold, 1, 0)

  # 處理實際標籤
  if (is.factor(data_df[[label_col]])) {
    truth <- as.numeric(data_df[[label_col]] == "canceled")
  } else {
    truth <- as.numeric(data_df[[label_col]])
  }

  acc <- Accuracy(y_pred = preds, y_true = truth)

  # Precision / Recall / F1
  prec <- rec <- f1 <- NA

  if (length(unique(preds)) > 1 && length(unique(truth)) > 1 && (1 %in% preds || 1 %in% truth)) {
    prec <- tryCatch(Precision(y_pred = factor(preds, levels = c(0, 1)),
                               y_true = factor(truth, levels = c(0, 1)), positive = "1"),
                     error = function(e) NA)
    rec <- tryCatch(Recall(y_pred = factor(preds, levels = c(0, 1)),
                           y_true = factor(truth, levels = c(0, 1)), positive = "1"),
                    error = function(e) NA)
    f1 <- tryCatch(F1_Score(y_pred = factor(preds, levels = c(0, 1)),
                            y_true = factor(truth, levels = c(0, 1)), positive = "1"),
                   error = function(e) NA)
  } else if (sum(truth == 1) == 0 && sum(preds == 1) == 0) {
    prec <- rec <- f1 <- 1
  } else if (sum(truth == 1) > 0 && sum(preds == 1) == 0) {
    prec <- rec <- f1 <- 0
  }

  # AUC
  auc_val <- NA
  valid_indices <- !is.na(probs) & !is.na(truth)
  if (sum(valid_indices) > 0 && length(unique(truth[valid_indices])) > 1) {
    roc_obj <- pROC::roc(response = truth[valid_indices], predictor = probs[valid_indices],
                         quiet = TRUE, levels = c(0, 1), direction = "<")
    auc_val <- auc(roc_obj)
  }

  return(data.frame(
    Accuracy = acc,
    Precision = prec,
    Recall = rec,
    F1_Score = f1,
    AUC = auc_val
  ))
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
# 將資料中的所有整數型數值轉成 numeric
convert_to_numeric <- function(df) {
  df[] <- lapply(df, function(col) {
    if (is.integer(col)) as.numeric(col) else col
  })
  return(df)
}

test_selected <- convert_to_numeric(test_selected)
```


```{r echo=FALSE, message=FALSE, warning=FALSE}
# ==== Step 9a: Evaluate caret model with custom metrics ====
optimal_threshold <- 0.65

# SMOTE 模型
if (!is.null(model_smote)) {
  test_results_smote <- evaluate_catboost_model(model_smote, test_selected, top_features, threshold = optimal_threshold)
} else {
  test_results_smote <- data.frame(Accuracy = NA, Precision = NA, Recall = NA, F1_Score = NA, AUC = NA)
}
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
optimal_threshold <- 0.55

# DownSample 模型
if (!is.null(model_downsample)) {
  test_results_downsample <- evaluate_catboost_model(model_downsample, test_selected, top_features, threshold = optimal_threshold)
} else {
  test_results_downsample <- data.frame(Accuracy = NA, Precision = NA, Recall = NA, F1_Score = NA, AUC = NA)
}
```

```{r}
# ==== Step 10: Compile Results Table ====

city_results <- data.frame(
  Hotel = "City Hotel",
  Method = rep(c("SMOTE", "DownSample"), each = 2),
  Dataset = rep(c("Validation", "Test"), 2),
  
  Accuracy = c(
    result_model_smote$cv_performance$Accuracy,
    test_results_smote$Accuracy,
    result_model_downsample$cv_performance$Accuracy,
    test_results_downsample$Accuracy
  ),
  
  Precision = c(
    result_model_smote$cv_performance$Precision,
    test_results_smote$Precision,
    result_model_downsample$cv_performance$Precision,
    test_results_downsample$Precision
  ),
  
  Recall = c(
    result_model_smote$cv_performance$Recall,
    test_results_smote$Recall,
    result_model_downsample$cv_performance$Recall,
    test_results_downsample$Recall
  ),
  
  F1_Score = c(
    result_model_smote$cv_performance$F1_Score,
    test_results_smote$F1_Score,
    result_model_downsample$cv_performance$F1_Score,
    test_results_downsample$F1_Score
  ),
  
  AUC = c(
    result_model_smote$cv_performance$AUC,
    test_results_smote$AUC,
    result_model_downsample$cv_performance$AUC,
    test_results_downsample$AUC
  )
)

# 儲存 CSV
write.csv(city_results, "result_city_cat_m2_SHAP.csv", row.names = FALSE)

# 顯示表格
kable(city_results,
      caption = "City Hotel Performance Comparison: Validation vs Test Sets",
      digits = 4)

```


```{r fig-pred-prob-dist, fig.cap="Prediction Probability Distribution for Canceled vs Not Canceled", warning=FALSE, message=FALSE}
library(ggplot2)

chosen_model <- model_smote
chosen_data <- test_selected

chosen_pool <- catboost.load_pool(
  data = chosen_data[, top_features, drop=FALSE]
)

chosen_probs <- catboost.predict(chosen_model, chosen_pool, prediction_type = "Probability")

truth_labels <- if (is.factor(chosen_data$is_canceled)) as.character(chosen_data$is_canceled) else as.character(factor(chosen_data$is_canceled, labels = c("not_canceled", "canceled")))

plot_df <- data.frame(prob = chosen_probs, truth = truth_labels)

ggplot(plot_df, aes(x = prob, fill = truth)) +
  geom_histogram(position = "identity", bins = 40, alpha = 0.6) +
  scale_fill_manual(values = c("not_canceled" = "#1f77b4", "canceled" = "#ff7f0e")) +
  labs(title = "Prediction Probability Distribution",
       x = "Predicted Probability for 'Canceled'",
       y = "Count",
       fill = "True Label") +
  theme_minimal()

```

 